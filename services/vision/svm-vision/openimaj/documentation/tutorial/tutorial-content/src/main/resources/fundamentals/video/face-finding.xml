<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="finding-faces">
  <title>Finding faces</title>
  <para>
    OpenIMAJ contains a set of classes that contain implementations of
    some of the state-of-the-art face detection and recognition
    algorithms. These classes are provided as a sub-project of the
    OpenIMAJ code-base called <filename>faces</filename>. The OpenIMAJ
    maven archetype adds the face library as a dependency and so we can
    start building face detection applications straight away.
  </para>
  <para>
    Create a new application using the quick-start archetype (see
    tutorial 1) and import it into your IDE. If you look at the
    <filename>pom.xml</filename> file you will see that the
    <code>faces</code> dependency from OpenIMAJ is already
    included. As you’ve already done the video-processing tutorial,
    we’ll try to find faces within the video that your cam produces.
    If you don’t have a cam, follow the video tutorial on how to use
    video from a file instead.
  </para>
  <para>
    Start by removing the code from the main method of the
    <filename>App.java</filename> class file. Then create a video capture
    object and a display to show the video. Create a listener on the
    video display to which we can hook our face finder. The code is
    below, but check out the previous tutorial on video processing if
    you’re not sure what it means.
  </para>
  <programlisting>VideoCapture vc = new VideoCapture( 320, 240 );
VideoDisplay&lt;MBFImage&gt; vd = VideoDisplay.createVideoDisplay( vc );
vd.addVideoListener( 
  new VideoDisplayListener&lt;MBFImage&gt;() {
    public void beforeUpdate( MBFImage frame ) {
    }

    public void afterUpdate( VideoDisplay&lt;MBFImage&gt; display ) {
    }
  });</programlisting>
  <para>
    For finding faces in images (or in this case video frames) we use a
    face detector. The <code>FaceDetector</code> interface
    provides the API for face detectors and there are currently two
    implementations within OpenIMAJ - the
    <code>HaarCascadeDetector</code> and the
    <code>SandeepFaceDetector</code>. The
    <code>HaarCascadeDetector</code> is considerably more robust
    than the <code>SandeepFaceDetector</code>, so we’ll use that.
  </para>
  <para>
    In the <code>beforeUpdate()</code> method, instantiate a new
    <code>HaarCascadeDetector</code>. The constructor takes the
    minimum size in pixels that a face can be detected at. For now, set
    this to <literal>40</literal> pixels:
  </para>
  <programlisting>FaceDetector&lt;DetectedFace,FImage&gt; fd = new HaarCascadeDetector(40);</programlisting>
  <para>
    Like all <code>FaceDetector</code> implementations, the
    <code>HaarCascadeDetector</code> has a method
    <code>detectFaces()</code> which takes an image. Because the
    <code>HaarCascadeDetector</code> uses single band images, we
    must convert our multi-band colour image into a single band image.
    To do this we can use the <code>Transforms</code> utility
    class that contains some static methods for converting images. The
    <code>calculateIntensity()</code> method will do just fine.
    Note that functionally the <code>calculateIntensity()</code>
    method does the same thing as the <code>flatten()</code>
    method we used earlier when used on RGB images.
  </para>
  <programlisting>List&lt;DetectedFace&gt; faces = fd.detectFaces(Transforms.calculateIntensity(frame));</programlisting>
  <para>
    The <code>detectFaces()</code> method returns a list of
    <code>DetectedFace</code> objects which contain information
    about the faces in the image. From these objects we can get the
    rectangular bounding boxes of each face and draw them back into our
    video frame. As we’re doing all this in our
    <code>beforeUpdate()</code> method, the video display will end
    up showing the bounding boxes on the displayed video. If you run the
    code and you have a cam attached, you should see yourself with a
    box drawn around your face. The complete code is shown below:
  </para>
  <programlisting>FaceDetector&lt;DetectedFace,FImage&gt; fd = new HaarCascadeDetector(40);
List&lt;DetectedFace&gt; faces = fd.detectFaces( Transforms.calculateIntensity(frame));

for( DetectedFace face : faces ) {
    frame.drawShape(face.getBounds(), RGBColour.RED);
}</programlisting>

<mediaobject>
  <imageobject>
		<imagedata fileref="../../figs/face-bounds.png" format="PNG" align="center" contentwidth="5cm"/>
  </imageobject>
</mediaobject>

  <para>
    OpenIMAJ has other face detectors which go a bit further than just
    finding the face. The <code>FKEFaceDetector</code> finds
    facial keypoints (the corners of the eyes, nose and mouth) and we
    can use this detector instead simply by instantiating that object
    instead of the <code>HaarCascadeDetector</code>. The
    <code>FKEFaceDetector</code> returns a slightly different
    object for each detected face, called a
    <code>KEDetectedFace</code>. The
    <code>KEDetectedFace</code> object contains the extra
    information about where the keypoints in the face are located. The
    lines of our code to instantiate the detector and detect faces can
    now be changed to the following:
  </para>
  <programlisting>FaceDetector&lt;KEDetectedFace,FImage&gt; fd = new FKEFaceDetector();
List&lt;KEDetectedFace&gt; faces = fd.detectFaces( Transforms.calculateIntensity( frame ) );</programlisting>
  <para>
    If you run the demo now, you will see exactly the same as before, as
    the <code>FKEFaceDetector</code> still detects bounding boxes.
    It may be running a bit slower though, as there is much more
    processing going on - we’re just not seeing the output of it! So,
    let’s plot the facial keypoints.
  </para>
  <para>
    To get the keypoints use <code>getKeypoints()</code> on the
    detected face. Each keypoint has a position (public field) which is
    relative to the face, so we’ll need to translate the point to the
    position of the face within the video frame before we plot the
    points. To do that we can use the <code>translate()</code>
    method of the <code>Point2d</code> class and the
    <code>minX()</code> and <code>minY()</code> methods of
    the <code>Rectangle</code> class.
  </para>
  <sect1 id="exercises-4">
    <title>Exercises</title>
    <sect2 id="exercise-1-drawing-facial-keypoints">
      <title>Exercise 1: Drawing facial keypoints</title>
      <para>
        Use the information above to plot the facial keypoints on the
        video.
      </para>

			<mediaobject>
			  <imageobject>
					<imagedata fileref="../../figs/face-points.png" format="PNG" align="center" contentwidth="5cm"/>
			  </imageobject>
			</mediaobject>

    </sect2>
    <sect2 id="exercise-2-speech-bubbles">
      <title>Exercise 2: Speech bubbles</title>
      <para>
        Try and take the speech bubble from the previous image tutorial
        and make it come from the mouth in the video.
        <emphasis role="strong">Hints:</emphasis> use
        <code>getKeypoint(FacialKeypointType)</code> to get the
        keypoint of the left corner of the mouth and plot the ellipses
        depending on that point. You may need to use smaller ellipses
        and text if your video is running at 320x240.
      </para>
			<mediaobject>
			  <imageobject>
					<imagedata fileref="../../figs/face-awesome.png" format="PNG" align="center" contentwidth="5cm"/>
			  </imageobject>
			</mediaobject>
    </sect2>
  </sect1>
</chapter>