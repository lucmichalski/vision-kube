{
 "metadata": {
  "name": "",
  "signature": "sha256:1b9dae40b3f1a9cadefa4887bea61c5ccf385a0720b865f7a05bbb66c8dbb6a9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import boto\n",
      "from boto.s3.connection import S3Connection\n",
      "\n",
      "from itertools import islice\n",
      "\n",
      "conn = S3Connection(anon=True)\n",
      "bucket = conn.get_bucket('aws-publicdatasets')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# a function to help with looking through S3 buckets\n",
      "\n",
      "from itertools import islice\n",
      "\n",
      "def keys_in_bucket(bucket, prefix, truncate_path=True, limit=None):\n",
      "    \"\"\"\n",
      "    given a S3 boto bucket, yields the keys in bucket with the prefix\n",
      "    if truncate_path is True, remove prefix in keys\n",
      "    optional limit to number of keys to return\n",
      "    \"\"\"\n",
      "    keys = islice(bucket.list(prefix=prefix, \n",
      "                                   delimiter=\"/\"),\n",
      "                       limit)\n",
      "    for key in keys:\n",
      "        name = key.name.encode(\"UTF-8\") \n",
      "        if truncate_path:\n",
      "            name = name.replace(prefix, \"\", 1)\n",
      "        yield name\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "What are the complete range of data for CC?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The top level bucket is `s3://aws-publicdatasets/common-crawl/`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# using boto to pull up the same list of buckets\n",
      "\n",
      "list(keys_in_bucket(bucket, \"common-crawl/\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Three eras of CC crawls \n",
      "\n",
      "within s3://aws-publicdatasets/common-crawl/\n",
      "\n",
      "I think of 3 phases:\n",
      "\n",
      "* pre-2012\n",
      "* 2012 corpus\n",
      "* Nutch era (after [transition to nutch](http://commoncrawl.org/common-crawl-move-to-nutch/) -- http://commoncrawl.org/new-crawl-data-available/)\n",
      "\n",
      "Let's look this up\n",
      "\n",
      "* ??? : crawl-001\n",
      "* ??? : crawl-002\n",
      "* 2012: parse-output  http://commoncrawl.org/2012-crawl-data-now-available/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pre - 2012\n",
      "\n",
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/crawl-001/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pre - 2012\n",
      "\n",
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/crawl-002/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 2012: https://commoncrawl.atlassian.net/wiki/display/CRWL/About+the+Data+Set\n",
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/parse-output/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/crawl-data/ | grep CC-MAIN"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "2012 Crawl"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for the 2012 data that I was working with in 2013.\n",
      "# not a good idea to take the contents for \"common-crawl/parse-output/segment/\" as valid segments\n",
      "# instead -- depend on s3://aws-publicdatasets/common-crawl/parse-output/valid_segments.txt\n",
      "\n",
      "# comment out so I won't run \n",
      "\n",
      "# for (i,key) in enumerate(islice(keys_in_bucket(bucket, \"common-crawl/parse-output/segment/\"),\n",
      "#                                 10)):\n",
      "#     print (i, key)\n",
      "    \n",
      "#!s3cmd ls s3://aws-publicdatasets/common-crawl/parse-output/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/parse-output/valid_segments.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = bucket.get_key(\"common-crawl/parse-output/valid_segments.txt\")\n",
      "s = k.get_contents_as_string()\n",
      "\n",
      "valid_segments = filter(None, s.split(\"\\n\"))\n",
      "\n",
      "print (len(valid_segments), valid_segments[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# April 2014 crawl\n",
      "\n",
      "\n",
      "http://commoncrawl.org/april-2014-crawl-data-available/\n",
      "\n",
      "*    all segments (CC-MAIN-2014-15/segment.paths.gz)\n",
      "*    all WARC files (CC-MAIN-2014-15/warc.paths.gz)\n",
      "*    all WAT files (CC-MAIN-2014-15/wat.paths.gz)\n",
      "*    all WET files (CC-MAIN-2014-15/wet.paths.gz)\n",
      "\n",
      "https://aws-publicdatasets.s3.amazonaws.com/common-crawl/crawl-data/CC-MAIN-2014-15/segment.paths.gz"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list(keys_in_bucket(bucket, \"common-crawl/crawl-data/CC-MAIN-2014-15/\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do all of the nutch era crawls follow the same structure?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[key for key in keys_in_bucket(bucket, \"common-crawl/crawl-data/\") if key.startswith(\"CC-MAIN\")]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# convert year/week number to datetime\n",
      "\n",
      "import datetime\n",
      "\n",
      "def week_in_year(year, week):\n",
      "    return datetime.datetime(year,1,1) + datetime.timedelta((week-1)*7)\n",
      "\n",
      "week_in_year(2014,15)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.maxint"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gzip\n",
      "import StringIO\n",
      "\n",
      "# the following functions \n",
      "\n",
      "def gzip_from_key(bucket, key_name):\n",
      "    k = bucket.get_key(key_name)\n",
      "    f = gzip.GzipFile(fileobj=StringIO.StringIO(k.get_contents_as_string()))\n",
      "    return f\n",
      "\n",
      "def segments_from_gzip(gz):\n",
      "    s = gz.read()\n",
      "    return filter(None, s.split(\"\\n\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# let's parse segment.paths.gz\n",
      "\n",
      "valid_segments = segments_from_gzip(gzip_from_key(bucket, \n",
      "                    \"common-crawl/crawl-data/CC-MAIN-2014-15/segment.paths.gz\"))\n",
      "valid_segments[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# rewrite to deal with gzip on the fly?\n",
      "# DON'T KNOW WHY THIS DOESN'T WORK -- COME BACK\n",
      "\n",
      "from boto.s3.key import Key\n",
      "from gzipstream import GzipStreamFile\n",
      "\n",
      "def gzip_from_key_2(bucket_name, key_name):\n",
      "\n",
      "    pds = conn.get_bucket(bucket_name)\n",
      "    k = Key(pds)\n",
      "    k.key = key_name    \n",
      "\n",
      "    f = GzipStreamFile(k)\n",
      "    \n",
      "    return f\n",
      "\n",
      "def segments_from_gzip_2(gz):\n",
      "    BUF_SIZE = 1000\n",
      "    s = \"\"\n",
      "    buffer = gz.read(BUF_SIZE)\n",
      "    while buffer:\n",
      "        s += buffer\n",
      "        buffer = gz.read(BUF_SIZE)\n",
      "\n",
      "    return filter(None, s.split(\"\\n\"))\n",
      "\n",
      "valid_segments_2 = segments_from_gzip_2(gzip_from_key_2(bucket, \n",
      "                    \"common-crawl/crawl-data/CC-MAIN-2014-15/segment.paths.gz\"))\n",
      "valid_segments_2[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "WAT:  metadata files"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "all WAT files (CC-MAIN-2014-15/wat.paths.gz)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wat_segments = segments_from_gzip(gzip_from_key(bucket, \n",
      "                    \"common-crawl/crawl-data/CC-MAIN-2014-15/wat.paths.gz\"))\n",
      "len(wat_segments)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wat_segments[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "http://commoncrawl.org/navigating-the-warc-file-format/\n",
      "\n",
      "json file\n",
      "\n",
      "forgot to check how big the file is before downloading it.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = bucket.get_key(wat_segments[0])\n",
      "k.size"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_key_sizes(bucket, keys):\n",
      "    sizes = []\n",
      "    for key in keys:\n",
      "        k = bucket.get_key(key)\n",
      "        sizes.append((key, k.size if hasattr(k, 'size') else 0))\n",
      "    return sizes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_key_sizes(bucket, wat_segments[0:20])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# http://stackoverflow.com/questions/2348317/how-to-write-a-pager-for-python-iterators/2350904#2350904        \n",
      "def grouper(iterable, page_size):\n",
      "    page= []\n",
      "    for item in iterable:\n",
      "        page.append( item )\n",
      "        if len(page) == page_size:\n",
      "            yield page\n",
      "            page= []\n",
      "    if len(page) > 0:\n",
      "        yield page\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "multiprocessing approach"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "when to use Processes vs Threads?  I think you use threads when there's no danger of memory collision\n",
      "\n",
      "http://sebastianraschka.com/Articles/2014_multiprocessing_intro.html#Multi-Threading-vs.-Multi-Processing:\n",
      "\n",
      ">  If we submit \"jobs\" to different threads, those jobs can be pictured as \"sub-tasks\" of a single process and those threads will usually have access to the same memory areas (i.e., shared memory). This approach can easily lead to conflicts in case of improper synchronization, for example, if processes are writing to the same memory location at the same time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wat_segments[0:2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "multiprocessing async approach"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# with pool.map\n",
      "\n",
      "from multiprocessing.dummy import Pool as ThreadPool \n",
      "from functools import partial\n",
      "\n",
      "get_key_sizes_for_bucket = partial(get_key_sizes, bucket)\n",
      "\n",
      "PAGE_SIZE = 1\n",
      "POOL_SIZE = 100\n",
      "MAX_SEGMENTS = 100\n",
      "\n",
      "pool = ThreadPool(POOL_SIZE) \n",
      "results = pool.map(get_key_sizes_for_bucket, grouper(wat_segments[0:MAX_SEGMENTS], PAGE_SIZE))\n",
      "pool.close()\n",
      "pool.join()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Pool.imap_unordered\n",
      "\n",
      "from multiprocessing.dummy import Pool as ThreadPool \n",
      "from functools import partial\n",
      "\n",
      "get_key_sizes_for_bucket = partial(get_key_sizes, bucket)\n",
      "\n",
      "PAGE_SIZE = 1\n",
      "POOL_SIZE = 100\n",
      "MAX_SEGMENTS = 500  # replace with None for all segments\n",
      "CHUNK_SIZE = 10\n",
      "\n",
      "pool = ThreadPool(POOL_SIZE) \n",
      "results_iter = pool.imap_unordered(get_key_sizes_for_bucket, \n",
      "                              grouper(wat_segments[0:MAX_SEGMENTS], PAGE_SIZE),\n",
      "                              CHUNK_SIZE)\n",
      "\n",
      "results = []\n",
      "                             \n",
      "for (i, page) in enumerate(islice(results_iter,None)):\n",
      "    print ('\\r>> Result %d' % i, end=\"\")\n",
      "    for result in page:\n",
      "        results.append(result)\n",
      "            \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import Series, DataFrame\n",
      "df = DataFrame(results, columns=['key', 'size'])\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df['size'].describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab --no-import-all inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pylab as P\n",
      "P.hist(df['size'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# total byte size of all the wat files\n",
      "print (format(sum(df['size']),\",d\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save results\n",
      "import csv\n",
      "with open('CC-MAIN-2014-15.wat.csv', 'wb') as csvfile:\n",
      "    wat_writer = csv.writer(csvfile, delimiter=',',\n",
      "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
      "    wat_writer.writerow(['key', 'size'])\n",
      "    for result in results:\n",
      "        wat_writer.writerow(result)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head CC-MAIN-2014-15.wat.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Taking apart a WAT file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this won't work for the version of boto I'm currently using -- a bug to be resolved\n",
      "\n",
      "key = bucket.get_key(wat_segments[0])\n",
      "url = key.generate_url(expires_in=0, query_auth=False)\n",
      "url"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wat_segments[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# looks like within any segment ID, we should expect warc, wat, wet buckets\n",
      "\n",
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2014-15/segments/1397609521512.15/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gzipstream import GzipStreamFile"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import boto\n",
      "from itertools import islice\n",
      "from boto.s3.key import Key\n",
      "from gzipstream import GzipStreamFile\n",
      "import warc\n",
      "import json\n",
      "\n",
      "\n",
      "def test_gzipstream():\n",
      "\n",
      "  output = []\n",
      "    \n",
      "  # Let's use a random gzipped web archive (WARC) file from the 2014-15 Common Crawl dataset\n",
      "  ## Connect to Amazon S3 using anonymous credentials\n",
      "  conn = boto.connect_s3(anon=True)\n",
      "  pds = conn.get_bucket('aws-publicdatasets')\n",
      "  ## Start a connection to one of the WARC files\n",
      "  k = Key(pds)\n",
      "  k.key = 'common-crawl/crawl-data/CC-MAIN-2014-15/segments/1397609521512.15/warc/CC-MAIN-20140416005201-00000-ip-10-147-4-33.ec2.internal.warc.gz'\n",
      "\n",
      "  # The warc library accepts file like objects, so let's use GzipStreamFile\n",
      "  f = warc.WARCFile(fileobj=GzipStreamFile(k))\n",
      "  for num, record in islice(enumerate(f),100):\n",
      "    if record['WARC-Type'] == 'response':\n",
      "      # Imagine we're interested in the URL, the length of content, and any Content-Type strings in there\n",
      "      output.append((record['WARC-Target-URI'], record['Content-Length']))\n",
      "      output.append( '\\n'.join(x for x in record.payload.read().replace('\\r', '').split('\\n\\n')[0].split('\\n') if 'content-type:' in x.lower()))\n",
      "      output.append( '=-=-' * 10)\n",
      " \n",
      "  return output\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_gzipstream()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def warc_records(key_name, limit=None):\n",
      "    conn = boto.connect_s3(anon=True)\n",
      "    bucket = conn.get_bucket('aws-publicdatasets')\n",
      "    key = bucket.get_key(key_name)\n",
      "\n",
      "    # The warc library accepts file like objects, so let's use GzipStreamFile\n",
      "    f = warc.WARCFile(fileobj=GzipStreamFile(key))\n",
      "    for record in islice(f, limit):\n",
      "        yield record"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# let's compute some stats on the headers\n",
      "from collections import Counter\n",
      "c=Counter()\n",
      "[c.update(record.header.keys()) for record in warc_records(wat_segments[0],1)]\n",
      "c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# warc-type\n",
      "\n",
      "# looks like \n",
      "# first record is 'warcinfo\n",
      "# rest is 'metadata'\n",
      "\n",
      "c=Counter()\n",
      "[c.update([record['warc-type']]) for record in warc_records(wat_segments[0],100)]\n",
      "\n",
      "c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# content-type\n",
      "\n",
      "c=Counter()\n",
      "[c.update([record['content-type']]) for record in warc_records(wat_segments[0],100)]\n",
      "\n",
      "c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# what"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wrecords = warc_records(wat_segments[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record = wrecords.next()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# http://warc.readthedocs.org/en/latest/#working-with-warc-header\n",
      "\n",
      "record.header.items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#payload\n",
      "dir(record.payload)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "record.header.get('content-type')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# payload\n",
      "\n",
      "\n",
      "s = record.payload.read()\n",
      "len(s)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if record.header.get('content-type') == 'application/json':\n",
      "    payload = json.loads(s)\n",
      "else:\n",
      "    payload = s\n",
      "    \n",
      "payload"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "payload['Envelope']['WARC-Header-Metadata']['WARC-Target-URI']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urlparse\n",
      "urlparse.urlparse(payload['Envelope']['WARC-Header-Metadata']['WARC-Target-URI']).netloc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# let's just run through a segment with minimal processing (to test processing speed)\n",
      "\n",
      "import urlparse\n",
      "\n",
      "def walk_through_segment(segment_id, limit=None):\n",
      "    \n",
      "    for (i,record) in enumerate(warc_records(segment_id,limit)):\n",
      "        print (\"\\r>> Record: %d\" % i, end=\"\")\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time walk_through_segment(wat_segments[0],100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urlparse\n",
      "\n",
      "def netloc_count_for_segment(segment_id, limit=None):\n",
      "    \n",
      "    c = Counter()\n",
      "\n",
      "    for (i, record) in enumerate(warc_records(segment_id,limit)):\n",
      "\n",
      "        print (\"\\r>>Record: %d\" % i, end=\"\")\n",
      "        s = record.payload.read()\n",
      "        if record.header.get('content-type') == 'application/json':\n",
      "            payload = json.loads(s)\n",
      "            url = payload['Envelope']['WARC-Header-Metadata'].get('WARC-Target-URI')\n",
      "            if url:\n",
      "                netloc = urlparse.urlparse(url).netloc\n",
      "                c.update([netloc])\n",
      "            else:\n",
      "                c.update([None])\n",
      "\n",
      "    return c\n",
      "\n",
      "\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time netloc_count_for_segment(wat_segments[0],1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# rewrite dropping Counter to use defaultdict\n",
      "# http://evanmuehlhausen.com/simple-counters-in-python-with-benchmarks/ suggests Counter is slow\n",
      "\n",
      "import urlparse\n",
      "from collections import defaultdict\n",
      "\n",
      "counter = defaultdict(int)\n",
      "\n",
      "def netloc_count_for_segment_dd(segment_id, limit=None):\n",
      "\n",
      "    c = defaultdict(int)\n",
      "    \n",
      "    for (i, record) in enumerate(warc_records(segment_id,limit)):\n",
      "\n",
      "        print (\"\\r>>Record: %d\" % i, end=\"\")\n",
      "        s = record.payload.read()\n",
      "        if record.header.get('content-type') == 'application/json':\n",
      "            payload = json.loads(s)\n",
      "            url = payload['Envelope']['WARC-Header-Metadata'].get('WARC-Target-URI')\n",
      "            if url:\n",
      "                netloc = urlparse.urlparse(url).netloc\n",
      "                c[netloc] += 1\n",
      "            else:\n",
      "                c[None] += 1\n",
      "\n",
      "    return c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time netloc_count_for_segment_dd(wat_segments[0],1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Tag Count"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# draw from https://github.com/Smerity/cc-mrjob/blob/7ab5a81ee698a2819ae1bc5295ac0de628f1ea6a/mrcc.py\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!s3cmd ls s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2014-23/warc.path.gz"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# WARC files from the latest crawl\n",
      "# s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2014-35/\n",
      "\n",
      "warc_segments = segments_from_gzip(gzip_from_key(bucket, \n",
      "                    \"common-crawl/crawl-data/CC-MAIN-2014-23/warc.path.gz\"))\n",
      "len(warc_segments)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "warc_segments[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "walk_through_segment(warc_segments[0],100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# code adapted from https://github.com/Smerity/cc-mrjob/blob/7ab5a81ee698a2819ae1bc5295ac0de628f1ea6a/tag_counter.py\n",
      "\n",
      "import re\n",
      "from collections import Counter\n",
      "\n",
      "# Optimization: compile the regular expression once so it's not done each time\n",
      "# The regular expression looks for (1) a tag name using letters (assumes lowercased input) and numbers\n",
      "# and (2) allows an optional for a space and then extra parameters, eventually ended by a closing >\n",
      "\n",
      "HTML_TAG_PATTERN = re.compile('<([a-z0-9]+)[^>]*>')\n",
      "\n",
      "def get_tag_count(data, ctr=None):\n",
      "  \"\"\"Extract the names and total usage count of all the opening HTML tags in the document\"\"\"\n",
      "  if ctr is None:\n",
      "    ctr = Counter()\n",
      "  # Convert the document to lower case as HTML tags are case insensitive\n",
      "  ctr.update(HTML_TAG_PATTERN.findall(data.lower()))\n",
      "  return ctr\n",
      "\n",
      "# Let's check to make sure the tag counter works as expected\n",
      "assert get_tag_count('<html><a href=\"...\"></a><h1 /><br/><p><p></p></p>') == {'html': 1, 'a': 1, 'p': 2, 'h1': 1, 'br': 1}\n",
      "\n",
      "\n",
      "def tag_counts_for_segment(segment_id, limit=None):\n",
      "    \n",
      "    ctr = Counter()\n",
      "    \n",
      "    for (i, record) in enumerate(warc_records(segment_id,limit)):\n",
      "\n",
      "        print (\"\\r>>Record: %d\" % i, end=\"\")\n",
      "        \n",
      "        if record.header.get('content-type') == 'application/http; msgtype=response':\n",
      "            payload = record.payload.read()\n",
      "            # The HTTP response is defined by a specification: first part is headers (metadata)\n",
      "            # and then following two CRLFs (newlines) has the data for the response\n",
      "            headers, body = payload.split('\\r\\n\\r\\n', 1)\n",
      "            if 'Content-Type: text/html' in headers:\n",
      "                # We avoid creating a new Counter for each page as that's actually quite slow\n",
      "                tag_count = get_tag_count(body,ctr)\n",
      "                \n",
      "                \n",
      "    return ctr\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# let's use multiprocessing to try to duplicate the list of segments on the mrjob example\n",
      "# https://raw.githubusercontent.com/commoncrawl/cc-mrjob/master/input/test-10.txt\n",
      "\n",
      "import requests\n",
      "mrjobs_segs = filter(None, \n",
      "                     requests.get(\"https://raw.githubusercontent.com/commoncrawl/cc-mrjob/master/input/test-10.txt\").content.split(\"\\n\"))\n",
      "mrjobs_segs "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from multiprocessing.dummy import Pool as ThreadPool \n",
      "from functools import partial\n",
      "\n",
      "\n",
      "def calc_tag_counts_for_segments(segments, max_segments=None, max_record_per_segment=None, pool_size=4, chunk_size=1,\n",
      "                                 print_result_count=False):\n",
      "    \n",
      "    \"\"\"\n",
      "    pool_size:  number of works in pool\n",
      "    max_segments:  number of segments to calculate (None for no limit)\n",
      "    max_record_per_segment:  max num of records to calculate per segment (None for no limit)\n",
      "    chunk_size: how to chunk jobs among pool workers\n",
      "    print_result_count: whether to print number of results available\n",
      "    \"\"\"\n",
      "    \n",
      "    tag_counts_for_segment_limit = partial(tag_counts_for_segment, limit=max_record_per_segment)\n",
      "\n",
      "\n",
      "    pool = ThreadPool(pool_size) \n",
      "    results_iter = pool.imap_unordered(tag_counts_for_segment_limit, \n",
      "                                  mrjobs_segs[0:max_segments],\n",
      "                                  chunk_size)\n",
      "\n",
      "    results = []\n",
      "    net_counts = Counter()\n",
      "\n",
      "    for (i, result) in enumerate(islice(results_iter,None)):\n",
      "        if print_result_count:\n",
      "            print ('\\r>> Result %d' % i, end=\"\")\n",
      "        results.append(result)\n",
      "        net_counts.update(result)\n",
      "    \n",
      "    return (net_counts, results)\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time tag_counts = calc_tag_counts_for_segments(mrjobs_segs, max_segments=None, max_record_per_segment=100, pool_size=4, chunk_size=1, print_result_count=True)[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Using IPython parallel"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can have multithreading within a given process but to use multiple processes, need to go with IPython parallel -- at least, that's what I've been able to work out"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import multiprocessing\n",
      "multiprocessing.cpu_count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython import parallel\n",
      "rc = parallel.Client()\n",
      "rc.block = True\n",
      "\n",
      "rc.ids"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To be continued...."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "WARC files:  the final frontier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Is it possible to work with pieces of the new warc files without having to download the whole file?  \n",
      "\n",
      "In the 2012 crawl data, there was an index built http://urlsearch.commoncrawl.org/ --> from which you can get a reference to an offset and length inside of the arc.gz file.  With S3, you don't need to then download the whole file but just a chunk....and that chunk itself is unpackable as a gzip file.  (nice feature of gzip?):\n",
      "\n",
      "http://nbviewer.ipython.org/github/rdhyee/working-open-data/blob/postscript/notebooks/Day_21_CommonCrawl_Content.ipynb#Grabbing-pieces-of-the-.arc.gz-files\n",
      "\n",
      "Is there a similar way of working with the 2014 crawl data?  That is, a way of reading off chunks of the warc file without grabbing entire warc files?"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}