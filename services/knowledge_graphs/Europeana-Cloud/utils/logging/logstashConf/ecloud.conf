input
{
	#this will be uncommented when kafka communication works
	#receive the messages from kafka
    kafka
	{
        zk_connect => "150.254.164.16:2181,150.254.164.17:2181,150.254.164.18:2181"
        topic_id => "elog"
        reset_beginning => false 
    }
	
	#uncomment for debugging
	#example input:
	#2014-06-10T12:10:20.123Z mcs mcs02 - WARN: We are loosing the touch. 
	stdin
	{

	}
}

filter{

	#First parsing. We want to check here if this message origins from LoggingFilter or not
	grok
	{
		match => [ "message", "%{TIMESTAMP_ISO8601:event_timestamp} %{WORD:category} %{HOSTNAME:instance} \[%{JAVACLASS:source}\] %{GREEDYDATA}" ]
	}	

	if [source] == "eu.europeana.cloud.service.commons.logging.LoggingFilter" {

		#Second parsing.
		grok
		{
			match => [ "message", "%{TIMESTAMP_ISO8601} %{WORD} %{HOSTNAME} \[%{JAVACLASS}\] - %{WORD:priority} : %{BASE10NUM:responseCode} %{BASE10NUM:responseDuration}ms %{IP:clientIP} %{URIPATH:requestPath} %{GREEDYDATA:data}" ]
			overwrite => [ "message" ]

		}

		#force internal logstash date to match the date included in message	
		date
		{
			match => [ "event_timestamp", "YYYY-MM-dd'T'HH:mm:ss.SSS'Z'"]
		}
		#then logstash will not allow us to use this date when naming files in 
		#due to: https://logstash.jira.com/browse/LOGSTASH-1340
		#so let's just extract it from message again 
		#(we will get the year as two digits, but I don't think it's an issue)
		grok
		{
			match => [ "message", "%{DATE:day}T%{GREEDYDATA}" ]
		}
		#remove the fields that we don't need so the solr won't error on them
		#we cannot remove filed '@timestamp', because it causes logstash to break
		#so we configured solr to receive this field and ignore it
		mutate
		{
			remove_field => [ "@version", "message", "kafka", "host","source"]
	    	}

		#after dropping original message field
		#we rename 'data' field to 'message' to use default Banana configuration
		mutate
		{
			rename => [ "data", "message" ]
		}

		#summarise the number of messages with ERROR and WARN priority
		if ( [priority] == "FATAL" or [priority] == "ERROR" or [priority] == "WARN" )
		{
			metrics
			{
				meter => "%{priority}.%{instance}"
				add_tag => "metric"
				flush_interval => 1
			}
		}
	
		#adjust the statistics to fit in 'ganglia' output format
		#and calculate 'diff' field from 'count' field
		#
		#incoming events from 'metrics' filter are canceled
		#new, multiple events are produced, with tag 'parsedMetric'
		#
		if "metric" in [tags]
		{
			metricsParser
			{
				field => "count"
			}
			
		}

	}else{
		grok
		{
			match => [ "message", "%{DATE:day}T%{GREEDYDATA}" ]
		}

		mutate {
			add_tag => [ "notIndexedMessage" ]
		}
	}
	
}

output 
{
	#uncomment for debugging
	stdout
	{
		codec => rubydebug
	}


	if "parsedMetric" in [tags]
	{
		ganglia
		{
			codec => "plain"
			
			#this is an example of using dynamic strings in group name
			#this is currently not possible
			#I created the pull request:
			#https://github.com/elasticsearch/logstash/pull/1465
			#
			#group => "%{category}_%{instance}_events"
			#
			#please note that metrics names still have to be globally unique,
			#even if they are used in different groups			

			group => "ecloud"
			host => "150.254.164.19"
			port => 8649
			metric => "%{instance}_%{priority}"
			metric_type => "uint32"
			value => "%{diff}"
			units => "events"
		}
	}

	
	if "notIndexedMessage" not in [tags]{
		if "_grokparsefailure" not in [tags]
		{

			solr_http
			{
				codec => rubydebug
				flush_size => 100
				idle_flush_time => 1
				solr_url => "http://localhost:8080/solr/core-today"
				workers => 1
			}
			file
			{
				message_format => "%{event_timestamp} %{category} %{instance} [eu.europeana.cloud.service.commons.logging.LoggingFilter] - %{priority}: %{responseCode} %{responseDuration}ms %{clientIP} %{requestPath} %{message}"
				#adjust the path as needed
				path => "/home/ecloud/logs/%{category}/%{instance}-%{day}.log"
			}
	
		}
		else
		{
			file
			{
				message_format => "%{message}"
				#adjust the path as needed
				path => "/home/ecloud/logs/%{category}/%{instance}-%{day}.log"
			}
		
		}
	}

	#write messages to file, one file per day per instance, separate folders for service types
	if "notIndexedMessage" in [tags]{
		file
		{
			message_format => "%{message}"
			#adjust the path as needed
			path => "/home/ecloud/logs/%{category}/%{instance}-%{day}.log"
		}
	}
}
